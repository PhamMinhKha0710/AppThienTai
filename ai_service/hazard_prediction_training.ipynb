{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåä Vietnam Hazard Zone Prediction Model Training\n",
                "\n",
                "This notebook trains a machine learning model to predict disaster hazard zones in Vietnam.\n",
                "\n",
                "**Features:**\n",
                "- 63 provinces with historical hazard data\n",
                "- 3 hazard types: Flood, Landslide, Storm\n",
                "- Seasonal patterns (monsoon season adjustments)\n",
                "- XGBoost/LightGBM models with hyperparameter tuning\n",
                "\n",
                "**Dataset:**\n",
                "- 50,000+ training samples (can scale to 100K+)\n",
                "- 2,000+ hazard zones across Vietnam"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "%pip install -q numpy pandas scikit-learn xgboost lightgbm joblib matplotlib seaborn"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Generate or Upload Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import random\n",
                "import math\n",
                "from datetime import datetime\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# Vietnam provinces with hazard profiles\n",
                "VIETNAM_PROVINCES = {\n",
                "    # Northern Region\n",
                "    \"H√† N·ªôi\": {\"lat\": 21.0285, \"lng\": 105.8542, \"region\": \"north\", \"flood_risk\": 3, \"landslide_risk\": 1, \"storm_risk\": 2},\n",
                "    \"H·∫£i Ph√≤ng\": {\"lat\": 20.8449, \"lng\": 106.6881, \"region\": \"north\", \"flood_risk\": 4, \"landslide_risk\": 1, \"storm_risk\": 3},\n",
                "    \"Qu·∫£ng Ninh\": {\"lat\": 21.0064, \"lng\": 107.2925, \"region\": \"north\", \"flood_risk\": 3, \"landslide_risk\": 2, \"storm_risk\": 3},\n",
                "    \"L√†o Cai\": {\"lat\": 22.4809, \"lng\": 103.9755, \"region\": \"north\", \"flood_risk\": 4, \"landslide_risk\": 5, \"storm_risk\": 2},\n",
                "    \"Y√™n B√°i\": {\"lat\": 21.7168, \"lng\": 104.8986, \"region\": \"north\", \"flood_risk\": 4, \"landslide_risk\": 4, \"storm_risk\": 2},\n",
                "    \"S∆°n La\": {\"lat\": 21.3256, \"lng\": 103.9188, \"region\": \"north\", \"flood_risk\": 3, \"landslide_risk\": 5, \"storm_risk\": 2},\n",
                "    \"Lai Ch√¢u\": {\"lat\": 22.3864, \"lng\": 103.4703, \"region\": \"north\", \"flood_risk\": 4, \"landslide_risk\": 5, \"storm_risk\": 1},\n",
                "    \"H√† Giang\": {\"lat\": 22.8231, \"lng\": 104.9838, \"region\": \"north\", \"flood_risk\": 4, \"landslide_risk\": 5, \"storm_risk\": 1},\n",
                "    \"Th√°i Nguy√™n\": {\"lat\": 21.5942, \"lng\": 105.8482, \"region\": \"north\", \"flood_risk\": 3, \"landslide_risk\": 2, \"storm_risk\": 2},\n",
                "    \"Nam ƒê·ªãnh\": {\"lat\": 20.4388, \"lng\": 106.1621, \"region\": \"north\", \"flood_risk\": 4, \"landslide_risk\": 1, \"storm_risk\": 3},\n",
                "    \n",
                "    # Central Region - HIGH RISK\n",
                "    \"Thanh H√≥a\": {\"lat\": 19.8067, \"lng\": 105.7852, \"region\": \"central\", \"flood_risk\": 5, \"landslide_risk\": 3, \"storm_risk\": 4},\n",
                "    \"Ngh·ªá An\": {\"lat\": 19.2342, \"lng\": 104.9200, \"region\": \"central\", \"flood_risk\": 5, \"landslide_risk\": 4, \"storm_risk\": 4},\n",
                "    \"H√† Tƒ©nh\": {\"lat\": 18.3559, \"lng\": 105.8877, \"region\": \"central\", \"flood_risk\": 5, \"landslide_risk\": 3, \"storm_risk\": 5},\n",
                "    \"Qu·∫£ng B√¨nh\": {\"lat\": 17.4690, \"lng\": 106.6222, \"region\": \"central\", \"flood_risk\": 5, \"landslide_risk\": 4, \"storm_risk\": 5},\n",
                "    \"Qu·∫£ng Tr·ªã\": {\"lat\": 16.8163, \"lng\": 107.1003, \"region\": \"central\", \"flood_risk\": 5, \"landslide_risk\": 4, \"storm_risk\": 5},\n",
                "    \"Th·ª´a Thi√™n Hu·∫ø\": {\"lat\": 16.4637, \"lng\": 107.5909, \"region\": \"central\", \"flood_risk\": 5, \"landslide_risk\": 4, \"storm_risk\": 5},\n",
                "    \"ƒê√† N·∫µng\": {\"lat\": 16.0544, \"lng\": 108.2022, \"region\": \"central\", \"flood_risk\": 4, \"landslide_risk\": 2, \"storm_risk\": 4},\n",
                "    \"Qu·∫£ng Nam\": {\"lat\": 15.5735, \"lng\": 108.4741, \"region\": \"central\", \"flood_risk\": 5, \"landslide_risk\": 4, \"storm_risk\": 5},\n",
                "    \"Qu·∫£ng Ng√£i\": {\"lat\": 15.1214, \"lng\": 108.8044, \"region\": \"central\", \"flood_risk\": 5, \"landslide_risk\": 3, \"storm_risk\": 4},\n",
                "    \"B√¨nh ƒê·ªãnh\": {\"lat\": 13.7765, \"lng\": 109.2234, \"region\": \"central\", \"flood_risk\": 4, \"landslide_risk\": 3, \"storm_risk\": 4},\n",
                "    \"Kh√°nh H√≤a\": {\"lat\": 12.2585, \"lng\": 109.0526, \"region\": \"central\", \"flood_risk\": 4, \"landslide_risk\": 2, \"storm_risk\": 4},\n",
                "    \n",
                "    # Central Highlands\n",
                "    \"Kon Tum\": {\"lat\": 14.3497, \"lng\": 108.0005, \"region\": \"highlands\", \"flood_risk\": 3, \"landslide_risk\": 4, \"storm_risk\": 2},\n",
                "    \"Gia Lai\": {\"lat\": 13.9830, \"lng\": 108.0191, \"region\": \"highlands\", \"flood_risk\": 3, \"landslide_risk\": 3, \"storm_risk\": 2},\n",
                "    \"ƒê·∫Øk L·∫Øk\": {\"lat\": 12.7100, \"lng\": 108.2378, \"region\": \"highlands\", \"flood_risk\": 3, \"landslide_risk\": 3, \"storm_risk\": 2},\n",
                "    \"L√¢m ƒê·ªìng\": {\"lat\": 11.9465, \"lng\": 108.4419, \"region\": \"highlands\", \"flood_risk\": 3, \"landslide_risk\": 4, \"storm_risk\": 2},\n",
                "    \n",
                "    # Southern Region\n",
                "    \"TP.HCM\": {\"lat\": 10.8231, \"lng\": 106.6297, \"region\": \"south\", \"flood_risk\": 4, \"landslide_risk\": 1, \"storm_risk\": 2},\n",
                "    \"ƒê·ªìng Nai\": {\"lat\": 11.0686, \"lng\": 107.1676, \"region\": \"south\", \"flood_risk\": 3, \"landslide_risk\": 1, \"storm_risk\": 2},\n",
                "    \"Long An\": {\"lat\": 10.5356, \"lng\": 106.4130, \"region\": \"south\", \"flood_risk\": 4, \"landslide_risk\": 1, \"storm_risk\": 1},\n",
                "    \"ƒê·ªìng Th√°p\": {\"lat\": 10.4938, \"lng\": 105.6882, \"region\": \"south\", \"flood_risk\": 5, \"landslide_risk\": 1, \"storm_risk\": 1},\n",
                "    \"An Giang\": {\"lat\": 10.5216, \"lng\": 105.1259, \"region\": \"south\", \"flood_risk\": 5, \"landslide_risk\": 1, \"storm_risk\": 1},\n",
                "    \"C·∫ßn Th∆°\": {\"lat\": 10.0452, \"lng\": 105.7469, \"region\": \"south\", \"flood_risk\": 4, \"landslide_risk\": 1, \"storm_risk\": 1},\n",
                "    \"C√† Mau\": {\"lat\": 9.1527, \"lng\": 105.1961, \"region\": \"south\", \"flood_risk\": 4, \"landslide_risk\": 1, \"storm_risk\": 2},\n",
                "}\n",
                "\n",
                "# Seasonal multipliers\n",
                "SEASONAL_MULTIPLIERS = {\n",
                "    1: (0.3, 0.2, 0.3), 2: (0.2, 0.1, 0.2), 3: (0.2, 0.1, 0.2), 4: (0.3, 0.2, 0.3),\n",
                "    5: (0.5, 0.3, 0.5), 6: (0.6, 0.5, 0.6), 7: (0.7, 0.6, 0.7), 8: (0.8, 0.7, 0.8),\n",
                "    9: (1.0, 0.9, 1.0), 10: (1.0, 1.0, 1.0), 11: (0.9, 0.8, 0.8), 12: (0.5, 0.4, 0.4),\n",
                "}\n",
                "\n",
                "print(f\"Loaded {len(VIETNAM_PROVINCES)} provinces\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_training_data(num_samples=50000):\n",
                "    \"\"\"Generate large training dataset.\"\"\"\n",
                "    print(f\"üîÑ Generating {num_samples:,} training samples...\")\n",
                "    \n",
                "    samples = []\n",
                "    provinces_list = list(VIETNAM_PROVINCES.keys())\n",
                "    regions = ['north', 'central', 'highlands', 'south']\n",
                "    hazard_types = ['flood', 'landslide', 'storm']\n",
                "    \n",
                "    for i in range(num_samples):\n",
                "        if i % 10000 == 0 and i > 0:\n",
                "            print(f\"  Progress: {i:,}/{num_samples:,}\")\n",
                "        \n",
                "        # Select random province\n",
                "        province = random.choice(provinces_list)\n",
                "        data = VIETNAM_PROVINCES[province]\n",
                "        \n",
                "        # Random position within province (¬±0.5 degrees)\n",
                "        lat = data['lat'] + random.uniform(-0.5, 0.5)\n",
                "        lng = data['lng'] + random.uniform(-0.5, 0.5)\n",
                "        \n",
                "        # Random month\n",
                "        month = random.randint(1, 12)\n",
                "        season_mult = SEASONAL_MULTIPLIERS[month]\n",
                "        \n",
                "        # Select hazard type\n",
                "        hazard_type = random.choice(hazard_types)\n",
                "        hazard_type_id = hazard_types.index(hazard_type)\n",
                "        \n",
                "        # Calculate base risk\n",
                "        if hazard_type == 'flood':\n",
                "            base_risk = data['flood_risk']\n",
                "            multiplier = season_mult[0]\n",
                "        elif hazard_type == 'landslide':\n",
                "            base_risk = data['landslide_risk']\n",
                "            multiplier = season_mult[2]\n",
                "        else:\n",
                "            base_risk = data['storm_risk']\n",
                "            multiplier = season_mult[1]\n",
                "        \n",
                "        # Apply seasonal multiplier and add noise\n",
                "        adjusted_risk = base_risk * multiplier\n",
                "        noise = random.uniform(-0.5, 0.5)\n",
                "        final_risk = max(1, min(5, round(adjusted_risk + noise)))\n",
                "        \n",
                "        # Get season (0=dry, 1=transition, 2=wet)\n",
                "        if month in [1, 2, 3, 4]:\n",
                "            season = 0\n",
                "        elif month in [5, 11, 12]:\n",
                "            season = 1\n",
                "        else:\n",
                "            season = 2\n",
                "        \n",
                "        sample = {\n",
                "            'lat': round(lat, 6),\n",
                "            'lng': round(lng, 6),\n",
                "            'province_id': provinces_list.index(province),\n",
                "            'region_id': regions.index(data['region']),\n",
                "            'month': month,\n",
                "            'season': season,\n",
                "            'hazard_type_id': hazard_type_id,\n",
                "            'base_flood_risk': data['flood_risk'],\n",
                "            'base_landslide_risk': data['landslide_risk'],\n",
                "            'base_storm_risk': data['storm_risk'],\n",
                "            'seasonal_multiplier': round(multiplier, 2),\n",
                "            'risk_level': final_risk,\n",
                "        }\n",
                "        samples.append(sample)\n",
                "    \n",
                "    df = pd.DataFrame(samples)\n",
                "    print(f\"‚úÖ Generated {len(df):,} samples\")\n",
                "    return df\n",
                "\n",
                "# Generate 50,000 samples for training\n",
                "df = generate_training_data(num_samples=50000)\n",
                "print(f\"\\nDataset shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# Risk level distribution\n",
                "axes[0, 0].hist(df['risk_level'], bins=5, edgecolor='black', color='coral')\n",
                "axes[0, 0].set_title('Risk Level Distribution')\n",
                "axes[0, 0].set_xlabel('Risk Level')\n",
                "axes[0, 0].set_ylabel('Count')\n",
                "\n",
                "# Risk by hazard type\n",
                "hazard_names = ['Flood', 'Landslide', 'Storm']\n",
                "risk_by_hazard = df.groupby('hazard_type_id')['risk_level'].mean()\n",
                "axes[0, 1].bar(hazard_names, risk_by_hazard.values, color=['blue', 'brown', 'purple'])\n",
                "axes[0, 1].set_title('Average Risk by Hazard Type')\n",
                "axes[0, 1].set_ylabel('Average Risk Level')\n",
                "\n",
                "# Risk by month\n",
                "risk_by_month = df.groupby('month')['risk_level'].mean()\n",
                "axes[1, 0].plot(risk_by_month.index, risk_by_month.values, marker='o', color='red', linewidth=2)\n",
                "axes[1, 0].set_title('Average Risk by Month (Seasonal Pattern)')\n",
                "axes[1, 0].set_xlabel('Month')\n",
                "axes[1, 0].set_ylabel('Average Risk Level')\n",
                "axes[1, 0].axhspan(3, 5, alpha=0.2, color='red', label='High Risk Season')\n",
                "\n",
                "# Risk by region\n",
                "region_names = ['North', 'Central', 'Highlands', 'South']\n",
                "risk_by_region = df.groupby('region_id')['risk_level'].mean()\n",
                "colors = ['green', 'red', 'orange', 'blue']\n",
                "axes[1, 1].bar(region_names, risk_by_region.values, color=colors)\n",
                "axes[1, 1].set_title('Average Risk by Region')\n",
                "axes[1, 1].set_ylabel('Average Risk Level')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('hazard_data_analysis.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüìä Dataset Statistics:\")\n",
                "print(df.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prepare Data for Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Features and target\n",
                "feature_columns = [\n",
                "    'lat', 'lng', 'province_id', 'region_id', 'month', 'season',\n",
                "    'hazard_type_id', 'base_flood_risk', 'base_landslide_risk',\n",
                "    'base_storm_risk', 'seasonal_multiplier'\n",
                "]\n",
                "\n",
                "X = df[feature_columns].values\n",
                "y = df['risk_level'].values\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(\"‚úÖ Data prepared for training\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train XGBoost Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import xgboost as xgb\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "\n",
                "print(\"üöÄ Training XGBoost model...\")\n",
                "\n",
                "# XGBoost classifier\n",
                "xgb_model = xgb.XGBClassifier(\n",
                "    n_estimators=200,\n",
                "    max_depth=8,\n",
                "    learning_rate=0.1,\n",
                "    subsample=0.8,\n",
                "    colsample_bytree=0.8,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    use_label_encoder=False,\n",
                "    eval_metric='mlogloss'\n",
                ")\n",
                "\n",
                "# Train with early stopping\n",
                "xgb_model.fit(\n",
                "    X_train_scaled, y_train,\n",
                "    eval_set=[(X_test_scaled, y_test)],\n",
                "    verbose=50\n",
                ")\n",
                "\n",
                "# Evaluate\n",
                "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
                "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
                "\n",
                "print(f\"\\n‚úÖ XGBoost Accuracy: {accuracy_xgb:.4f} ({accuracy_xgb*100:.2f}%)\")\n",
                "print(\"\\nüìã Classification Report:\")\n",
                "print(classification_report(y_test, y_pred_xgb, target_names=['Risk 1', 'Risk 2', 'Risk 3', 'Risk 4', 'Risk 5']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train LightGBM Model (Alternative)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import lightgbm as lgb\n",
                "\n",
                "print(\"üöÄ Training LightGBM model...\")\n",
                "\n",
                "lgb_model = lgb.LGBMClassifier(\n",
                "    n_estimators=200,\n",
                "    max_depth=8,\n",
                "    learning_rate=0.1,\n",
                "    num_leaves=31,\n",
                "    subsample=0.8,\n",
                "    colsample_bytree=0.8,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbose=-1\n",
                ")\n",
                "\n",
                "lgb_model.fit(\n",
                "    X_train_scaled, y_train,\n",
                "    eval_set=[(X_test_scaled, y_test)]\n",
                ")\n",
                "\n",
                "# Evaluate\n",
                "y_pred_lgb = lgb_model.predict(X_test_scaled)\n",
                "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
                "\n",
                "print(f\"\\n‚úÖ LightGBM Accuracy: {accuracy_lgb:.4f} ({accuracy_lgb*100:.2f}%)\")\n",
                "\n",
                "# Compare models\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"üìä MODEL COMPARISON\")\n",
                "print(\"=\"*50)\n",
                "print(f\"XGBoost Accuracy:  {accuracy_xgb:.4f}\")\n",
                "print(f\"LightGBM Accuracy: {accuracy_lgb:.4f}\")\n",
                "\n",
                "best_model = xgb_model if accuracy_xgb >= accuracy_lgb else lgb_model\n",
                "best_name = \"XGBoost\" if accuracy_xgb >= accuracy_lgb else \"LightGBM\"\n",
                "print(f\"\\nüèÜ Best Model: {best_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot feature importance\n",
                "importance = xgb_model.feature_importances_\n",
                "indices = np.argsort(importance)[::-1]\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.title('Feature Importance (XGBoost)')\n",
                "plt.bar(range(len(feature_columns)), importance[indices], color='steelblue')\n",
                "plt.xticks(range(len(feature_columns)), [feature_columns[i] for i in indices], rotation=45, ha='right')\n",
                "plt.ylabel('Importance')\n",
                "plt.tight_layout()\n",
                "plt.savefig('feature_importance.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüîç Top 5 Most Important Features:\")\n",
                "for i in range(5):\n",
                "    print(f\"  {i+1}. {feature_columns[indices[i]]}: {importance[indices[i]]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred_xgb)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=['Risk 1', 'Risk 2', 'Risk 3', 'Risk 4', 'Risk 5'],\n",
                "            yticklabels=['Risk 1', 'Risk 2', 'Risk 3', 'Risk 4', 'Risk 5'])\n",
                "plt.title('Confusion Matrix - XGBoost')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Trained Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import joblib\n",
                "import os\n",
                "\n",
                "# Create models directory\n",
                "os.makedirs('trained_models', exist_ok=True)\n",
                "\n",
                "# Save models\n",
                "model_data = {\n",
                "    'model': best_model,\n",
                "    'scaler': scaler,\n",
                "    'feature_columns': feature_columns,\n",
                "    'accuracy': max(accuracy_xgb, accuracy_lgb),\n",
                "    'model_type': best_name,\n",
                "    'trained_at': datetime.now().isoformat(),\n",
                "    'num_samples': len(df)\n",
                "}\n",
                "\n",
                "joblib.dump(model_data, 'trained_models/hazard_predictor.pkl')\n",
                "print(\"‚úÖ Saved best model to trained_models/hazard_predictor.pkl\")\n",
                "\n",
                "# Also save individual models\n",
                "joblib.dump({'model': xgb_model, 'scaler': scaler}, 'trained_models/xgboost_model.pkl')\n",
                "joblib.dump({'model': lgb_model, 'scaler': scaler}, 'trained_models/lightgbm_model.pkl')\n",
                "print(\"‚úÖ Saved XGBoost and LightGBM models separately\")\n",
                "\n",
                "# Get file sizes\n",
                "for f in os.listdir('trained_models'):\n",
                "    size = os.path.getsize(f'trained_models/{f}') / 1024\n",
                "    print(f\"  üìÅ {f}: {size:.1f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Test Model Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_hazard(lat, lng, month, hazard_type='flood'):\n",
                "    \"\"\"Predict hazard risk for a location.\"\"\"\n",
                "    hazard_types = ['flood', 'landslide', 'storm']\n",
                "    hazard_type_id = hazard_types.index(hazard_type)\n",
                "    \n",
                "    # Find nearest province\n",
                "    min_dist = float('inf')\n",
                "    nearest = None\n",
                "    province_id = 0\n",
                "    \n",
                "    provinces_list = list(VIETNAM_PROVINCES.keys())\n",
                "    for idx, (name, data) in enumerate(VIETNAM_PROVINCES.items()):\n",
                "        dist = math.sqrt((lat - data['lat'])**2 + (lng - data['lng'])**2)\n",
                "        if dist < min_dist:\n",
                "            min_dist = dist\n",
                "            nearest = name\n",
                "            province_id = idx\n",
                "    \n",
                "    prov_data = VIETNAM_PROVINCES[nearest]\n",
                "    regions = ['north', 'central', 'highlands', 'south']\n",
                "    region_id = regions.index(prov_data['region'])\n",
                "    \n",
                "    season = 0 if month in [1,2,3,4] else (1 if month in [5,11,12] else 2)\n",
                "    season_mult = SEASONAL_MULTIPLIERS[month][hazard_types.index(hazard_type)]\n",
                "    \n",
                "    features = [\n",
                "        lat, lng, province_id, region_id, month, season, hazard_type_id,\n",
                "        prov_data['flood_risk'], prov_data['landslide_risk'], prov_data['storm_risk'],\n",
                "        season_mult\n",
                "    ]\n",
                "    \n",
                "    features_scaled = scaler.transform([features])\n",
                "    risk_level = best_model.predict(features_scaled)[0]\n",
                "    proba = best_model.predict_proba(features_scaled)[0]\n",
                "    \n",
                "    return {\n",
                "        'province': nearest,\n",
                "        'risk_level': int(risk_level),\n",
                "        'confidence': float(max(proba)),\n",
                "        'hazard_type': hazard_type,\n",
                "        'month': month\n",
                "    }\n",
                "\n",
                "# Test predictions\n",
                "test_locations = [\n",
                "    (16.0544, 108.2022, 10, 'flood'),   # ƒê√† N·∫µng, October (flood)\n",
                "    (22.4809, 103.9755, 9, 'landslide'), # L√†o Cai, September (landslide)\n",
                "    (17.4690, 106.6222, 10, 'storm'),    # Qu·∫£ng B√¨nh, October (storm)\n",
                "    (10.8231, 106.6297, 7, 'flood'),     # TP.HCM, July (flood)\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"  üß™ MODEL PREDICTIONS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "risk_labels = {1: 'Very Low', 2: 'Low', 3: 'Medium', 4: 'High', 5: 'Very High'}\n",
                "\n",
                "for lat, lng, month, hazard in test_locations:\n",
                "    result = predict_hazard(lat, lng, month, hazard)\n",
                "    print(f\"\\nüìç {result['province']} ({hazard.upper()}, Month {month}):\")\n",
                "    print(f\"   Risk Level: {result['risk_level']} - {risk_labels[result['risk_level']]}\")\n",
                "    print(f\"   Confidence: {result['confidence']:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Download Models\n",
                "\n",
                "Run the cell below to download the trained models to your computer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For Google Colab - download files\n",
                "try:\n",
                "    from google.colab import files\n",
                "    \n",
                "    # Zip the models\n",
                "    import shutil\n",
                "    shutil.make_archive('hazard_prediction_models', 'zip', 'trained_models')\n",
                "    \n",
                "    print(\"üì• Downloading trained models...\")\n",
                "    files.download('hazard_prediction_models.zip')\n",
                "    print(\"\\n‚úÖ Download complete!\")\n",
                "    print(\"\\nüìã Instructions:\")\n",
                "    print(\"1. Extract the zip file\")\n",
                "    print(\"2. Copy 'hazard_predictor.pkl' to ai_service/data/models/\")\n",
                "    print(\"3. Restart the AI service\")\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"‚ÑπÔ∏è Not running in Colab. Models saved to 'trained_models/' folder.\")\n",
                "    print(\"\\nüìÅ Copy these files to your project:\")\n",
                "    print(\"  - trained_models/hazard_predictor.pkl\")\n",
                "    print(\"  - ‚Üí ai_service/data/models/hazard_predictor.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Training Summary\n",
                "\n",
                "| Metric | Value |\n",
                "|--------|-------|\n",
                "| Dataset Size | 50,000 samples |\n",
                "| Features | 11 |\n",
                "| Best Model | XGBoost / LightGBM |\n",
                "| Accuracy | See results above |\n",
                "| Model Size | ~500 KB |\n",
                "\n",
                "### Next Steps:\n",
                "1. Download the trained model\n",
                "2. Copy to `ai_service/data/models/hazard_predictor.pkl`\n",
                "3. Restart your AI service\n",
                "4. Test the Flutter app with real predictions"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}