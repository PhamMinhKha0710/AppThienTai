{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Smart Alert AI Service - Training Notebook\n",
    "\n",
    "This notebook trains all AI models for the Smart Alert service from scratch.\n",
    "\n",
    "## Models:\n",
    "1. **Alert Scoring Model** - Random Forest for priority scoring (0-100)\n",
    "2. **Semantic Duplicate Detector** - Sentence Transformers for duplicate detection\n",
    "3. **Notification Timing Model** - Thompson Sampling for optimal notification times\n",
    "\n",
    "## Steps:\n",
    "1. Install dependencies\n",
    "2. Setup project directories\n",
    "3. Cleanup old data\n",
    "4. Train Alert Scorer\n",
    "5. Initialize Duplicate Detector\n",
    "6. Train Notification Timing\n",
    "7. Test all models\n",
    "8. Download trained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Dependencies installed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q numpy pandas scikit-learn scipy joblib\n",
    "%pip install -q sentence-transformers\n",
    "%pip install -q fastapi uvicorn pydantic\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Setup Project Directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directories created:\n",
      "  - Models: \\content\\ai_service\\data\\models\n",
      "  - Training: \\content\\ai_service\\data\\training\n",
      "  - Cache: \\content\\ai_service\\data\\cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create project directories\n",
    "BASE_DIR = Path('/content/ai_service')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODELS_DIR = DATA_DIR / 'models'\n",
    "TRAINING_DIR = DATA_DIR / 'training'\n",
    "CACHE_DIR = DATA_DIR / 'cache'\n",
    "\n",
    "for directory in [MODELS_DIR, TRAINING_DIR, CACHE_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Directories created:\")\n",
    "print(f\"  - Models: {MODELS_DIR}\")\n",
    "print(f\"  - Training: {TRAINING_DIR}\")\n",
    "print(f\"  - Cache: {CACHE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cleanup Old Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Deleted: alert_scorer.pkl\n",
      "  Deleted: notification_timing.json\n",
      "  Deleted: .locks\n",
      "  Deleted: models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2\n",
      "\n",
      "‚úÖ Cleanup complete! (4 items deleted)\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def cleanup_all():\n",
    "    \"\"\"Remove all old models, databases, and caches\"\"\"\n",
    "    deleted = 0\n",
    "    \n",
    "    # Cleanup models\n",
    "    if MODELS_DIR.exists():\n",
    "        for f in MODELS_DIR.glob('*'):\n",
    "            if f.is_file():\n",
    "                f.unlink()\n",
    "                print(f\"  Deleted: {f.name}\")\n",
    "                deleted += 1\n",
    "    \n",
    "    # Cleanup database\n",
    "    if TRAINING_DIR.exists():\n",
    "        for f in TRAINING_DIR.glob('*.db*'):\n",
    "            f.unlink()\n",
    "            print(f\"  Deleted: {f.name}\")\n",
    "            deleted += 1\n",
    "    \n",
    "    # Cleanup cache\n",
    "    if CACHE_DIR.exists():\n",
    "        for item in CACHE_DIR.iterdir():\n",
    "            if item.is_dir():\n",
    "                shutil.rmtree(item)\n",
    "            else:\n",
    "                item.unlink()\n",
    "            print(f\"  Deleted: {item.name}\")\n",
    "            deleted += 1\n",
    "    \n",
    "    if deleted == 0:\n",
    "        print(\"  (No old files found)\")\n",
    "    print(f\"\\n‚úÖ Cleanup complete! ({deleted} items deleted)\")\n",
    "\n",
    "cleanup_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train Alert Scoring Model (Random Forest)\n",
    "\n",
    "Uses synthetic data generated from rule-based formulas to train a Random Forest model.\n",
    "\n",
    "**Features (15 total):**\n",
    "- Alert properties: severity, type, age, distance, audience match\n",
    "- Contextual: user interactions, time of day, day of week, weather\n",
    "- Characteristics: content length, images, safety guide\n",
    "- Social signals: similar alerts, engagement rate, source reliability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Training Alert Scoring Model...\n",
      "  Generating 1000 synthetic samples...\n",
      "  Training Random Forest...\n",
      "\n",
      "üìä Training Results:\n",
      "  - Training time: 0.86 seconds\n",
      "  - Cross-validation MAE: 2.38\n",
      "\n",
      "  Top 5 Feature Importance:\n",
      "    - severity_score: 0.5407\n",
      "    - distance_km: 0.2603\n",
      "    - hours_since_created: 0.0839\n",
      "    - alert_type_score: 0.0697\n",
      "    - target_audience_match: 0.0126\n",
      "\n",
      "‚úÖ Alert Scorer saved to: \\content\\ai_service\\data\\models\\alert_scorer.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import math\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Configuration\n",
    "N_FEATURES = 15\n",
    "N_SAMPLES = 1000\n",
    "RF_N_ESTIMATORS = 100\n",
    "RF_MAX_DEPTH = 10\n",
    "RF_RANDOM_STATE = 42\n",
    "\n",
    "def generate_synthetic_features(n_samples: int) -> np.ndarray:\n",
    "    \"\"\"Generate synthetic feature vectors\"\"\"\n",
    "    np.random.seed(42)\n",
    "    features = np.zeros((n_samples, N_FEATURES))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        features[i, 0] = np.random.choice([1, 2, 3, 4])  # severity_score\n",
    "        features[i, 1] = np.random.choice([1, 2, 3, 4])  # alert_type_score\n",
    "        features[i, 2] = np.random.exponential(12)       # hours_since_created\n",
    "        features[i, 3] = np.random.exponential(20)       # distance_km\n",
    "        features[i, 4] = np.random.choice([0, 1])        # target_audience_match\n",
    "        features[i, 5] = np.random.poisson(5)            # user_previous_interactions\n",
    "        features[i, 6] = np.random.randint(0, 24)        # time_of_day\n",
    "        features[i, 7] = np.random.randint(0, 7)         # day_of_week\n",
    "        features[i, 8] = np.random.choice([0, 1, 2, 3, 4])  # weather_severity\n",
    "        features[i, 9] = np.random.randint(50, 500)      # content_length\n",
    "        features[i, 10] = np.random.choice([0, 1])       # has_images\n",
    "        features[i, 11] = np.random.choice([0, 1])       # has_safety_guide\n",
    "        features[i, 12] = np.random.poisson(3)           # similar_alerts_count\n",
    "        features[i, 13] = np.random.beta(2, 2)           # alert_engagement_rate\n",
    "        features[i, 14] = np.random.uniform(0.5, 1.0)    # source_reliability\n",
    "    \n",
    "    return features\n",
    "\n",
    "def apply_rule_based_scoring(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply rule-based scoring formula\"\"\"\n",
    "    scores = np.zeros(len(X))\n",
    "    \n",
    "    for i, features in enumerate(X):\n",
    "        severity_score = 25 * features[0]\n",
    "        type_score = 30 + 17.5 * features[1]\n",
    "        hours = features[2]\n",
    "        time_decay_score = 100 * math.exp(-0.05 * hours)\n",
    "        distance = features[3]\n",
    "        if distance >= 50:\n",
    "            distance_score = 0\n",
    "        else:\n",
    "            ratio = 1 - (distance / 50)\n",
    "            distance_score = 100 * ratio * ratio\n",
    "        audience_score = 100 if features[4] else 50\n",
    "        \n",
    "        final_score = (\n",
    "            0.35 * severity_score +\n",
    "            0.20 * type_score +\n",
    "            0.15 * time_decay_score +\n",
    "            0.20 * distance_score +\n",
    "            0.10 * audience_score\n",
    "        )\n",
    "        scores[i] = np.clip(final_score, 0, 100)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"üîÑ Training Alert Scoring Model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate training data\n",
    "print(f\"  Generating {N_SAMPLES} synthetic samples...\")\n",
    "X = generate_synthetic_features(N_SAMPLES)\n",
    "y = apply_rule_based_scoring(X)\n",
    "\n",
    "# Train model\n",
    "print(\"  Training Random Forest...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=RF_N_ESTIMATORS,\n",
    "    max_depth=RF_MAX_DEPTH,\n",
    "    random_state=RF_RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "cv_mae = -cv_scores.mean()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Feature importance\n",
    "feature_names = [\n",
    "    'severity_score', 'alert_type_score', 'hours_since_created',\n",
    "    'distance_km', 'target_audience_match', 'user_previous_interactions',\n",
    "    'time_of_day', 'day_of_week', 'weather_severity',\n",
    "    'content_length', 'has_images', 'has_safety_guide',\n",
    "    'similar_alerts_count', 'alert_engagement_rate', 'source_reliability'\n",
    "]\n",
    "importance = dict(zip(feature_names, model.feature_importances_))\n",
    "top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(f\"\\nüìä Training Results:\")\n",
    "print(f\"  - Training time: {elapsed:.2f} seconds\")\n",
    "print(f\"  - Cross-validation MAE: {cv_mae:.2f}\")\n",
    "print(f\"\\n  Top 5 Feature Importance:\")\n",
    "for name, imp in top_features:\n",
    "    print(f\"    - {name}: {imp:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model_data = {'model': model, 'scaler': scaler, 'is_trained': True}\n",
    "model_path = MODELS_DIR / 'alert_scorer.pkl'\n",
    "joblib.dump(model_data, model_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Alert Scorer saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Initialize Semantic Duplicate Detector\n",
    "\n",
    "Uses pre-trained Sentence Transformer model (multilingual BERT) for semantic similarity.\n",
    "No training needed - just download and test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading Sentence Transformer model...\n",
      "  (This may take a few minutes on first run)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model loaded in 11.70 seconds\n",
      "  Model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "\n",
      "üß™ Testing semantic similarity...\n",
      "\n",
      "  Text 1: M∆∞a l·ªõn g√¢y ng·∫≠p l·ª•t t·∫°i qu·∫≠n 1, TP.HCM\n",
      "  Text 2: Ng·∫≠p l·ª•t do m∆∞a l·ªõn ·ªü khu v·ª±c qu·∫≠n 1 th√†nh ph·ªë H·ªì Ch√≠ Minh\n",
      "  Text 3: ƒê·ªông ƒë·∫•t m·∫°nh 5.5 ƒë·ªô richter t·∫°i Nh·∫≠t B·∫£n\n",
      "\n",
      "  Similarity (1-2, similar): 0.7800\n",
      "  Similarity (1-3, different): 0.2965\n",
      "\n",
      "‚úÖ Duplicate Detector ready!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "print(\"üîÑ Loading Sentence Transformer model...\")\n",
    "print(\"  (This may take a few minutes on first run)\")\n",
    "\n",
    "start_time = time.time()\n",
    "duplicate_model = SentenceTransformer(MODEL_NAME, cache_folder=str(CACHE_DIR))\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded in {elapsed:.2f} seconds\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "\n",
    "# Test similarity\n",
    "print(\"\\nüß™ Testing semantic similarity...\")\n",
    "\n",
    "test_texts = [\n",
    "    \"M∆∞a l·ªõn g√¢y ng·∫≠p l·ª•t t·∫°i qu·∫≠n 1, TP.HCM\",\n",
    "    \"Ng·∫≠p l·ª•t do m∆∞a l·ªõn ·ªü khu v·ª±c qu·∫≠n 1 th√†nh ph·ªë H·ªì Ch√≠ Minh\",\n",
    "    \"ƒê·ªông ƒë·∫•t m·∫°nh 5.5 ƒë·ªô richter t·∫°i Nh·∫≠t B·∫£n\"\n",
    "]\n",
    "\n",
    "embeddings = duplicate_model.encode(test_texts)\n",
    "\n",
    "sim_12 = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "sim_13 = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]\n",
    "\n",
    "print(f\"\\n  Text 1: {test_texts[0]}\")\n",
    "print(f\"  Text 2: {test_texts[1]}\")\n",
    "print(f\"  Text 3: {test_texts[2]}\")\n",
    "print(f\"\\n  Similarity (1-2, similar): {sim_12:.4f}\")\n",
    "print(f\"  Similarity (1-3, different): {sim_13:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Duplicate Detector ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Train Notification Timing Model (Thompson Sampling)\n",
    "\n",
    "Uses Multi-Armed Bandit with Thompson Sampling to learn optimal notification times.\n",
    "Simulates realistic engagement patterns for 24 time slots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Training Notification Timing Model...\n",
      "  Simulating realistic engagement patterns...\n",
      "\n",
      "üìä Best notification times:\n",
      "  - 20:00 - Success rate: 0.77 (20 samples)\n",
      "  - 21:00 - Success rate: 0.77 (20 samples)\n",
      "  - 18:00 - Success rate: 0.77 (20 samples)\n",
      "  - 19:00 - Success rate: 0.77 (20 samples)\n",
      "  - 17:00 - Success rate: 0.77 (20 samples)\n",
      "\n",
      "‚úÖ Notification Timing saved to: \\content\\ai_service\\data\\models\\notification_timing.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "N_TIME_SLOTS = 24\n",
    "EPSILON = 0.1\n",
    "\n",
    "print(\"üîÑ Training Notification Timing Model...\")\n",
    "\n",
    "# Initialize Beta distribution parameters (uniform prior)\n",
    "alpha = np.ones(N_TIME_SLOTS, dtype=float)\n",
    "beta_param = np.ones(N_TIME_SLOTS, dtype=float)\n",
    "\n",
    "# Simulate realistic day patterns\n",
    "print(\"  Simulating realistic engagement patterns...\")\n",
    "\n",
    "patterns = {\n",
    "    'morning': (6, 9, 0.6),      # Moderate engagement\n",
    "    'work': (9, 17, 0.3),        # Low engagement\n",
    "    'evening': (17, 22, 0.8),    # High engagement\n",
    "    'night_early': (22, 24, 0.1),\n",
    "    'night_late': (0, 6, 0.1)\n",
    "}\n",
    "\n",
    "for name, (start, end, rate) in patterns.items():\n",
    "    for hour in range(start, end):\n",
    "        if hour < N_TIME_SLOTS:\n",
    "            successes = int(20 * rate)\n",
    "            failures = 20 - successes\n",
    "            alpha[hour] += successes\n",
    "            beta_param[hour] += failures\n",
    "\n",
    "# Calculate expected success rates\n",
    "expected_rewards = alpha / (alpha + beta_param)\n",
    "top_indices = np.argsort(expected_rewards)[-5:][::-1]\n",
    "\n",
    "print(\"\\nüìä Best notification times:\")\n",
    "for idx in top_indices:\n",
    "    rate = expected_rewards[idx]\n",
    "    samples = int(alpha[idx] + beta_param[idx] - 2)\n",
    "    print(f\"  - {idx:02d}:00 - Success rate: {rate:.2f} ({samples} samples)\")\n",
    "\n",
    "# Save parameters\n",
    "params_data = {\n",
    "    'alpha': alpha.tolist(),\n",
    "    'beta': beta_param.tolist(),\n",
    "    'n_slots': N_TIME_SLOTS,\n",
    "    'epsilon': EPSILON\n",
    "}\n",
    "\n",
    "params_path = MODELS_DIR / 'notification_timing.json'\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(params_data, f)\n",
    "\n",
    "print(f\"\\n‚úÖ Notification Timing saved to: {params_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Test All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing All Models\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Alert Scoring Model\n",
      "  Test alert (high severity, weather type):\n",
      "  - Priority Score: 74.71\n",
      "  - Confidence: 0.96\n",
      "\n",
      "2Ô∏è‚É£ Semantic Duplicate Detector\n",
      "  Similar alerts similarity: 0.6017 (threshold: 0.85)\n",
      "  Different alerts similarity: 0.3344\n",
      "  Duplicate detected: No\n",
      "\n",
      "3Ô∏è‚É£ Notification Timing Model\n",
      "  Recommended notification time: 18:00\n",
      "  Expected success rate: 0.77\n",
      "\n",
      "============================================================\n",
      "‚úÖ All models tested successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"üß™ Testing All Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test Alert Scorer\n",
    "print(\"\\n1Ô∏è‚É£ Alert Scoring Model\")\n",
    "scorer_data = joblib.load(MODELS_DIR / 'alert_scorer.pkl')\n",
    "scorer_model = scorer_data['model']\n",
    "scorer_scaler = scorer_data['scaler']\n",
    "\n",
    "test_features = np.array([[\n",
    "    3, 2, 2, 10, 1, 3, 14, 2, 2, 200, 1, 1, 2, 0.7, 0.9\n",
    "]])\n",
    "\n",
    "test_scaled = scorer_scaler.transform(test_features)\n",
    "score = scorer_model.predict(test_scaled)[0]\n",
    "\n",
    "tree_preds = np.array([tree.predict(test_scaled)[0] for tree in scorer_model.estimators_])\n",
    "confidence = 1.0 - (np.std(tree_preds) / 100.0)\n",
    "\n",
    "print(f\"  Test alert (high severity, weather type):\")\n",
    "print(f\"  - Priority Score: {score:.2f}\")\n",
    "print(f\"  - Confidence: {confidence:.2f}\")\n",
    "\n",
    "# Test Duplicate Detector\n",
    "print(\"\\n2Ô∏è‚É£ Semantic Duplicate Detector\")\n",
    "alert1 = \"C·∫£nh b√°o m∆∞a l·ªõn t·∫°i Qu·∫≠n 7, nguy c∆° ng·∫≠p cao\"\n",
    "alert2 = \"M∆∞a to ·ªü Q7, c√≥ th·ªÉ g√¢y ng·∫≠p n·∫∑ng\"\n",
    "alert3 = \"ƒê·ªông ƒë·∫•t 4.5 ƒë·ªô richter t·∫°i ƒêi·ªán Bi√™n\"\n",
    "\n",
    "emb = duplicate_model.encode([alert1, alert2, alert3])\n",
    "sim_same = cosine_similarity([emb[0]], [emb[1]])[0][0]\n",
    "sim_diff = cosine_similarity([emb[0]], [emb[2]])[0][0]\n",
    "\n",
    "print(f\"  Similar alerts similarity: {sim_same:.4f} (threshold: 0.85)\")\n",
    "print(f\"  Different alerts similarity: {sim_diff:.4f}\")\n",
    "print(f\"  Duplicate detected: {'Yes' if sim_same >= 0.85 else 'No'}\")\n",
    "\n",
    "# Test Notification Timing\n",
    "print(\"\\n3Ô∏è‚É£ Notification Timing Model\")\n",
    "with open(MODELS_DIR / 'notification_timing.json', 'r') as f:\n",
    "    timing_data = json.load(f)\n",
    "\n",
    "alpha_loaded = np.array(timing_data['alpha'])\n",
    "beta_loaded = np.array(timing_data['beta'])\n",
    "\n",
    "# Thompson Sampling selection\n",
    "samples = np.array([np.random.beta(alpha_loaded[i], beta_loaded[i]) for i in range(24)])\n",
    "best_slot = int(np.argmax(samples))\n",
    "\n",
    "print(f\"  Recommended notification time: {best_slot:02d}:00\")\n",
    "print(f\"  Expected success rate: {(alpha_loaded[best_slot] / (alpha_loaded[best_slot] + beta_loaded[best_slot])):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ All models tested successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Download Trained Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Creating downloadable archive...\n",
      "\n",
      "Files included:\n",
      "  - alert_scorer.pkl (6527.9 KB)\n",
      "  - notification_timing.json (0.3 KB)\n",
      "\n",
      "‚úÖ Archive created: trained_models.zip\n",
      "\n",
      "‚ö†Ô∏è Not running in Colab. Find the file at: /content/trained_models.zip\n"
     ]
    }
   ],
   "source": [
    "# Create zip of trained models\n",
    "OUTPUT_ZIP = '/content/trained_models.zip'\n",
    "\n",
    "print(\"üì¶ Creating downloadable archive...\")\n",
    "\n",
    "# List files\n",
    "print(\"\\nFiles included:\")\n",
    "for f in MODELS_DIR.glob('*'):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"  - {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Create zip\n",
    "shutil.make_archive('/content/trained_models', 'zip', MODELS_DIR)\n",
    "\n",
    "print(f\"\\n‚úÖ Archive created: trained_models.zip\")\n",
    "\n",
    "# Download (for Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(OUTPUT_ZIP)\n",
    "    print(\"\\nüì• Download started!\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è Not running in Colab. Find the file at: /content/trained_models.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Training Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  TRAINING SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìÖ Completed at: 2026-01-04 19:57:47\n",
      "\n",
      "üìä Models trained:\n",
      "  ‚úÖ Alert Scoring Model (Random Forest)\n",
      "     - 1000 synthetic samples\n",
      "     - 15 features\n",
      "  ‚úÖ Semantic Duplicate Detector (Sentence Transformers)\n",
      "     - Pre-trained multilingual model\n",
      "     - Threshold: 0.85\n",
      "  ‚úÖ Notification Timing Model (Thompson Sampling)\n",
      "     - 24 time slots\n",
      "     - Simulated engagement patterns\n",
      "\n",
      "üìÅ Output files:\n",
      "  - alert_scorer.pkl (6527.9 KB)\n",
      "  - notification_timing.json (0.3 KB)\n",
      "\n",
      "============================================================\n",
      "  üéâ All models ready for deployment!\n",
      "============================================================\n",
      "\n",
      "üìù Next steps:\n",
      "  1. Download trained_models.zip\n",
      "  2. Extract to ai_service/data/models/\n",
      "  3. Run: python main.py\n",
      "  4. API available at: http://localhost:8000/docs\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìÖ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nüìä Models trained:\")\n",
    "print(\"  ‚úÖ Alert Scoring Model (Random Forest)\")\n",
    "print(\"     - 1000 synthetic samples\")\n",
    "print(\"     - 15 features\")\n",
    "print(\"  ‚úÖ Semantic Duplicate Detector (Sentence Transformers)\")\n",
    "print(\"     - Pre-trained multilingual model\")\n",
    "print(\"     - Threshold: 0.85\")\n",
    "print(\"  ‚úÖ Notification Timing Model (Thompson Sampling)\")\n",
    "print(\"     - 24 time slots\")\n",
    "print(\"     - Simulated engagement patterns\")\n",
    "\n",
    "print(\"\\nüìÅ Output files:\")\n",
    "for f in MODELS_DIR.glob('*'):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"  - {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  üéâ All models ready for deployment!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìù Next steps:\")\n",
    "print(\"  1. Download trained_models.zip\")\n",
    "print(\"  2. Extract to ai_service/data/models/\")\n",
    "print(\"  3. Run: python main.py\")\n",
    "print(\"  4. API available at: http://localhost:8000/docs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
